{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Probabilities and Non Linearity \n",
    "\n",
    "**Activation Function** - is one when applied to a neurons in a layer during prediction that decides which neurons are activated. Relu function is one that we have used earlier. \n",
    "\n",
    "The properties that activation functions must adhere too: \n",
    "1. Function has to be continuous and inifinite in domain. There cannot be a value from which the activation func does not return a value. \n",
    "2. good activitation functions are monotonic that means they never change direction, i.e. always increasing or always decreasing. So a parabolic curve is never a good activation function because it is not monotonic and we have 2 values of x that give same value of y. \n",
    "3. Good activation functions are non linear which means they turn. This is needed for the purpose of **sometimes corelation** for a NN. \n",
    "4. Good activiation functions (and their derivatives) must be efficiently computable. - this is because this functions will be called millions of time during training therefore they must be calculable fast. \n",
    "5. Most activation functions are differentiable. This is required in the case of gradient based calculations where the back propagation requires the activation function has to be differentiated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions \n",
    "1. Sigmoid - bread and butter activation functions very popular along with relu  \n",
    "![sigmoid](images/sigmoid.png)\n",
    "2. Tanh - this is good for hiddern layers as it can throw negative corelations.  \n",
    "![tanh](images/tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden vs Output Layer Activation functions \n",
    "\n",
    "### 1. Predicting Raw data values \n",
    "Example the values we have to predict are not binary e.g. predicting the temperature of a region given the temperatures of the area in an around the location. Here we do not need a activation function that tries to narrow the values to a 0/1. \n",
    "\n",
    "### 2. Predicting Yes/No probabilties \n",
    "Here Sigmoid functions are the best. This is also good when we have a matrix that can be broken down to a series of 0/1 and matrix of data (o/1) represents the entire data sets. This is also good in the case of hidden layers as they are generally probabilities of 0/1 \n",
    "\n",
    "### 3. Predicting \"which one\" probabilities (softmax) \n",
    "This is generally used when you have to predict a single label out of a possible many in the set. E.g. recognizing handwritten numbers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax \n",
    "Softmax activation function is e(x) which firstly makes all numbers positive. It make smaller numbers not that big howeve, make large numbers very large. After the calculation of e(x) we calculate the sum of all the parameter values and then divide the individual parameters with the sum. This makes smaller values small and large values stand out. This essentuates \"sharpness of attenuation\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
