{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplification of the Model \n",
    "\n",
    "here we try and simplify the model of layers and weights into a model representing lego blocks. \n",
    "\n",
    "![Simplification](images/simplificaiton-model.png)\n",
    "\n",
    "In the diagram above we can see that the layers and weights are nothing but a series of matrices that can need to multipled in order to get output. Weights are adjusted to reduce error and with the help of backpropagation we can reduce the error and get to the right weights across all layers. \n",
    "\n",
    "The architecture shown above is a called a Feed forward architecture as we have layers and wights that make up the architecture and data moves forward. \n",
    "\n",
    "One of the key motivations of Deep learning is to find new architectures that can help get to a corelation between inputs and outputs quicker and faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplying notation \n",
    "Now that we have a simple way of visualizing the NN we can also use algebric notation to represent the NN. \n",
    "If we denote each weight matrix with Wi and layer as li then we can represent the neural network with the following algebra notation. \n",
    "\n",
    "![algebric-notation](images/algebra-notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that presents all visualizations and formulas \n",
    "\n",
    "![All-notations](images/all-notations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final conclusion is that different architectures are required for different problem sets to make use of Deep Learning. \n",
    "Image data sets will require a different architecture from a text problems set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization | Stopping and Drop out \n",
    "\n",
    "Regularization is a technique to avoid overfitting and that is done by stopping and early drop out in the process of learning. This helps model not to memorize the data points but learn from them in other words **\" A subset of methods that help your neural network learn the signal and ignore the noise.\"**\n",
    "\n",
    "![regularization](images/stop-early-dropout.png)\n",
    "\n",
    "Idea is to stop training as soon as we see a drop in test set accuracy to avoid overfitting. This is the cheapest form or regularization. \n",
    "\n",
    "In order to do this we need to divide our dataset into 3: \n",
    "1. Training Set - this is the data on which the model is trained. \n",
    "2. Validation Set - this is the data on which we look at the accuracy of the model trained on the training set. This is the set on which stopping of the training is done. \n",
    "3. Testing set - this is the set on which the model can be tested for accuracy at the end.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
